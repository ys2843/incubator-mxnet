% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mxnet_generated.R
\name{mx.nd.ftml.update}
\alias{mx.nd.ftml.update}
\title{The FTML optimizer described in
*FTML - Follow the Moving Leader in Deep Learning*,
available at http://proceedings.mlr.press/v70/zheng17a/zheng17a.pdf.}
\arguments{
\item{weight}{NDArray-or-Symbol
Weight}

\item{grad}{NDArray-or-Symbol
Gradient}

\item{d}{NDArray-or-Symbol
Internal state ``d_t``}

\item{v}{NDArray-or-Symbol
Internal state ``v_t``}

\item{z}{NDArray-or-Symbol
Internal state ``z_t``}

\item{lr}{float, required
Learning rate.}

\item{beta1}{float, optional, default=0.600000024
Generally close to 0.5.}

\item{beta2}{float, optional, default=0.999000013
Generally close to 1.}

\item{epsilon}{double, optional, default=9.9999999392252903e-09
Epsilon to prevent div 0.}

\item{t}{int, required
Number of update.}

\item{wd}{float, optional, default=0
Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.}

\item{rescale.grad}{float, optional, default=1
Rescale gradient to grad = rescale_grad*grad.}

\item{clip.grad}{float, optional, default=-1
Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient <= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).}
}
\value{
out The result mx.ndarray
}
\description{
.. math::
}
\details{
g_t = \nabla J(W_{t-1})\\
 v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\
 d_t = \frac{ 1 - \beta_1^t }{ \eta_t } (\sqrt{ \frac{ v_t }{ 1 - \beta_2^t } } + \epsilon)
 \sigma_t = d_t - \beta_1 d_{t-1}
 z_t = \beta_1 z_{ t-1 } + (1 - \beta_1^t) g_t - \sigma_t W_{t-1}
 W_t = - \frac{ z_t }{ d_t }



Defined in src/operator/optimizer_op.cc:L631
}
