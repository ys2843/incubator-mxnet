% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mxnet_generated.R
\name{mx.nd.mp.lamb.update.phase1}
\alias{mx.nd.mp.lamb.update.phase1}
\title{Mixed Precision version of Phase I of lamb update 
it performs the following operations and returns g:.}
\arguments{
\item{weight}{NDArray-or-Symbol
Weight}

\item{grad}{NDArray-or-Symbol
Gradient}

\item{mean}{NDArray-or-Symbol
Moving mean}

\item{var}{NDArray-or-Symbol
Moving variance}

\item{weight32}{NDArray-or-Symbol
Weight32}

\item{beta1}{float, optional, default=0.899999976
The decay rate for the 1st moment estimates.}

\item{beta2}{float, optional, default=0.999000013
The decay rate for the 2nd moment estimates.}

\item{epsilon}{float, optional, default=9.99999997e-07
A small constant for numerical stability.}

\item{t}{int, required
Index update count.}

\item{bias.correction}{boolean, optional, default=1
Whether to use bias correction.}

\item{wd}{float, required
Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.}

\item{rescale.grad}{float, optional, default=1
Rescale gradient to grad = rescale_grad*grad.}

\item{clip.gradient}{float, optional, default=-1
Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient <= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).}
}
\value{
out The result mx.ndarray
}
\description{
Link to paper: https://arxiv.org/pdf/1904.00962.pdf
}
\details{
.. math::
              \begin{gather*}
              grad32 = grad(float16) * rescale_grad
              if (grad < -clip_gradient)
              then
                   grad = -clip_gradient
              if (grad > clip_gradient)
              then
                   grad = clip_gradient

              mean = beta1 * mean + (1 - beta1) * grad;
              variance = beta2 * variance + (1. - beta2) * grad ^ 2;

              if (bias_correction)
              then
                   mean_hat = mean / (1. - beta1^t);
                   var_hat = var / (1 - beta2^t);
                   g = mean_hat / (var_hat^(1/2) + epsilon) + wd * weight32;
              else
                   g = mean / (var_data^(1/2) + epsilon) + wd * weight32;
              \end{gather*}

          

Defined in src/operator/optimizer_op.cc:L1024
}
