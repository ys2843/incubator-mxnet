% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mxnet_generated.R
\name{mx.symbol.multi_mp_sgd_mom_update}
\alias{mx.symbol.multi_mp_sgd_mom_update}
\title{multi_mp_sgd_mom_update:Momentum update function for multi-precision Stochastic Gradient Descent (SGD) optimizer.}
\usage{
mx.symbol.multi_mp_sgd_mom_update(...)
}
\arguments{
\item{data}{NDArray-or-Symbol[]
Weights}

\item{lrs}{tuple of <float>, required
Learning rates.}

\item{wds}{tuple of <float>, required
Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.}

\item{momentum}{float, optional, default=0
The decay rate of momentum estimates at each epoch.}

\item{rescale.grad}{float, optional, default=1
Rescale gradient to grad = rescale_grad*grad.}

\item{clip.gradient}{float, optional, default=-1
Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient <= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).}

\item{num.weights}{int, optional, default='1'
Number of updated weights.}

\item{name}{string, optional
Name of the resulting symbol.}
}
\value{
out The result mx.symbol
}
\description{
Momentum update has better convergence rates on neural networks. Mathematically it looks
like below:
}
\details{
.. math::

  v_1 = \alpha * \nabla J(W_0)\\
  v_t = \gamma v_{t-1} - \alpha * \nabla J(W_{t-1})\\
  W_t = W_{t-1} + v_t

It updates the weights using::

  v = momentum * v - learning_rate * gradient
  weight += v

Where the parameter ``momentum`` is the decay rate of momentum estimates at each epoch.



Defined in src/operator/optimizer_op.cc:L472
}
