% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mxnet_generated.R
\name{mx.nd.Activation}
\alias{mx.nd.Activation}
\title{Applies an activation function element-wise to the input.}
\arguments{
\item{data}{NDArray-or-Symbol
The input array.}

\item{act.type}{{'relu', 'sigmoid', 'softrelu', 'softsign', 'tanh'}, required
Activation function to be applied.}
}
\value{
out The result mx.ndarray
}
\description{
The following activation functions are supported:
}
\details{
- `relu`: Rectified Linear Unit, :math:`y = max(x, 0)`
- `sigmoid`: :math:`y = \frac{1}{1 + exp(-x)}`
- `tanh`: Hyperbolic tangent, :math:`y = \frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}`
- `softrelu`: Soft ReLU, or SoftPlus, :math:`y = log(1 + exp(x))`
- `softsign`: :math:`y = \frac{x}{1 + abs(x)}`



Defined in src/operator/nn/activation.cc:L168
}
